<!doctype html>
<html>
<head>
    <meta charset="utf-8" />
    <title>ONNX Runtime Web — Demo</title>
    <style>
        body { font-family: sans-serif; padding: 16px; }
        #wrap { position: relative; display: inline-block; }
        canvas { display:block; }
        #overlay { position: absolute; left: 0; top: 0; pointer-events: none; }
        #controls { margin-top: 12px; }
    </style>
</head>
<body>
<h3>ONNX Runtime Web — Browser inference + draw boxes</h3>

<input type="file" id="imgfile" accept="image/*"/>
<div id="wrap">
    <canvas id="imageCanvas"></canvas>
    <canvas id="overlay"></canvas>
</div>
<div id="controls">
    <button id="runBtn">Run Inference</button>
    <span id="log"></span>
</div>

<!-- onnxruntime-web from unpkg (WASM backend by default) -->
<script src="https://unpkg.com/onnxruntime-web/dist/ort.min.js"></script>

<script>
  // 设置 —— 根据你的模型调整
  const MODEL_URL = './yolov8n.onnx'; // 把你的模型放在同目录或用绝对 URL
  const MODEL_INPUT_SIZE = 640;     // 例如 YOLOv8 常用 640
  const SCORE_THRESHOLD = 0.3;
  const IOU_THRESHOLD = 0.45;
  const CLASS_NAMES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light",
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle",
    "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange",
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed",
    "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven",
    "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
  ]; // 按你模型的类别填写

  // 元素
  const imgFile = document.getElementById('imgfile');
  const imgCanvas = document.getElementById('imageCanvas');
  const overlay = document.getElementById('overlay');
  const runBtn = document.getElementById('runBtn');
  const logEl = document.getElementById('log');

  let session = null;
  let loadedImage = null;

  // 加载模型（一次）
  async function loadModel() {
    log('Loading model...');
    // 可选择后端: ort.env.wasm.numThreads / ort.env.webgl.asyncThreshold 等（见文档）
    session = await ort.InferenceSession.create(MODEL_URL);
    log('Model loaded.');
  }

  // 显示图片到 canvas（并调整 overlay 大小）
  function drawImageToCanvas(img) {
    const ctx = imgCanvas.getContext('2d');
    const w = img.naturalWidth;
    const h = img.naturalHeight;
    imgCanvas.width = w;
    imgCanvas.height = h;
    overlay.width = w;
    overlay.height = h;
    ctx.drawImage(img, 0, 0);
  }

  imgFile.addEventListener('change', (e) => {
    const file = e.target.files[0];
    if (!file) return;
    const url = URL.createObjectURL(file);
    const img = new Image();
    img.onload = () => {
      loadedImage = img;
      drawImageToCanvas(img);
    };
    img.src = url;
  });

  runBtn.addEventListener('click', async () => {
    if (!loadedImage) return alert('请选择图片');
    if (!session) await loadModel();
    await runInferenceOnImage(loadedImage);
  });

  // 预处理：letterbox -> resize -> to Float32Array (NCHW)
  function preprocess(img, targetSize) {
    const tmp = document.createElement('canvas');
    tmp.width = targetSize;
    tmp.height = targetSize;
    const tctx = tmp.getContext('2d');

    // letterbox 算法（保持长宽比，填充灰色）
    const iw = img.naturalWidth, ih = img.naturalHeight;
    const scale = Math.min(targetSize / iw, targetSize / ih);
    const nw = Math.round(iw * scale), nh = Math.round(ih * scale);
    const dx = Math.floor((targetSize - nw) / 2);
    const dy = Math.floor((targetSize - nh) / 2);

    tctx.fillStyle = 'rgb(114,114,114)';
    tctx.fillRect(0,0,targetSize,targetSize);
    tctx.drawImage(img, 0,0, iw,ih, dx,dy, nw,nh);

    const imgData = tctx.getImageData(0,0,targetSize,targetSize);
    const data = imgData.data;
    // 转 Float32Array, NCHW, 0..1 或归一化到 [-1,1] 视模型而定
    const floats = new Float32Array(3 * targetSize * targetSize);
    // 用常见的 BGR或RGB顺序视模型要求调整；这里假设模型需要 RGB, 0..1
    let ptr = 0;
    for (let c = 0; c < 3; c++) { // R,G,B
      for (let y=0; y<targetSize; y++){
        for (let x=0; x<targetSize; x++){
          const i = (y*targetSize + x)*4;
          // r g b 顺序 (data[i+0] = r)
          const val = data[i + c] / 255.0;
          floats[ptr++] = val;
        }
      }
    }
    return {input: floats, scale, dx, dy, iw, ih};
  }

  // 简单 NMS (IOU)
  function nms(boxes, scores, iouThreshold){
    const idxs = Array.from(scores.map((s,i)=>[s,i]))
      .sort((a,b)=>b[0]-a[0])
      .map(x=>x[1]);
    const selected = [];
    while (idxs.length){
      const i = idxs.shift();
      selected.push(i);
      for (let j = idxs.length -1; j>=0; j--){
        const k = idxs[j];
        if (iou(boxes[i], boxes[k]) > iouThreshold) idxs.splice(j,1);
      }
    }
    return selected;
  }
  function iou(a,b){
    const x1 = Math.max(a[0], b[0]);
    const y1 = Math.max(a[1], b[1]);
    const x2 = Math.min(a[2], b[2]);
    const y2 = Math.min(a[3], b[3]);
    const inter = Math.max(0, x2-x1) * Math.max(0, y2-y1);
    const areaA = (a[2]-a[0])*(a[3]-a[1]);
    const areaB = (b[2]-b[0])*(b[3]-b[1]);
    return inter / (areaA + areaB - inter);
  }

  // 解析模型输出 —— **非常依赖模型**，这里给出YOLO-like的一般思路
  function parseYOLOOutput(output, meta){
    // 假设 output 是 1D Float32Array，按检测行排列: [cx,cy,w,h, obj_conf, cls0,cls1,...]
    // 你需要依据具体模型来写这个解析器（不同 ONNX/导出方式会不同）。
    const boxes = [], scores = [], classes = [];
    const numParamsPerBox = meta.paramsPerBox; // 4 + 1 + numClasses
    for (let i=0; i < output.length; i += numParamsPerBox) {
      const cx = output[i], cy = output[i+1], w = output[i+2], h = output[i+3];
      const objConf = output[i+4];
      // class scores start at i+5
      let maxClass = -1, maxScore = 0;
      for (let c=0; c < meta.numClasses; c++){
        const sc = output[i + 5 + c];
        if (sc > maxScore){ maxScore = sc; maxClass = c; }
      }
      const conf = objConf * maxScore;
      if (conf > SCORE_THRESHOLD){
        // 转为 x1,y1,x2,y2（相对于输入图像 ratio）
        const x1 = cx - w/2, y1 = cy - h/2, x2 = cx + w/2, y2 = cy + h/2;
        boxes.push([x1, y1, x2, y2]);
        scores.push(conf);
        classes.push(maxClass);
      }
    }
    return {boxes, scores, classes};
  }

  // 绘制结果到 overlay canvas（输入是相对模型输入的 ratio 或 pixel - 这里做缩放）
  function drawDetections(dets, meta, prep) {
    const ctx = overlay.getContext('2d');
    ctx.clearRect(0,0,overlay.width, overlay.height);
    ctx.lineWidth = 2;
    ctx.font = '16px sans-serif';
    for (let i=0;i<dets.boxes.length;i++){
      const b = dets.boxes[i];
      const cls = dets.classes[i];
      const score = dets.scores[i];
      // b 是基于模型输入大小 (0..targetSize)，需要映射回原图尺寸
      // 用 letterbox 参数反映射（dx,dy,scale）
      const x1 = (b[0] - prep.dx) / prep.scale;
      const y1 = (b[1] - prep.dy) / prep.scale;
      const x2 = (b[2] - prep.dx) / prep.scale;
      const y2 = (b[3] - prep.dy) / prep.scale;
      ctx.strokeStyle = 'red';
      ctx.fillStyle = 'red';
      ctx.strokeRect(x1, y1, x2-x1, y2-y1);
      const label = `${CLASS_NAMES[cls] || cls} ${(score*100).toFixed(1)}%`;
      const textW = ctx.measureText(label).width;
      ctx.fillRect(x1, y1 - 18, textW + 8, 18);
      ctx.fillStyle = 'white';
      ctx.fillText(label, x1 + 4, y1 - 4);
    }
  }

  async function runInferenceOnImage(img) {
    log('Preprocessing...');
    const prep = preprocess(img, MODEL_INPUT_SIZE);
    // 创建 tensor: shape [1,3,H,W]
    const inputTensor = new ort.Tensor('float32', prep.input, [1,3,MODEL_INPUT_SIZE,MODEL_INPUT_SIZE]);

    log('Running session...');
    const feeds = {};
    // 请替换为你的模型输入名（可通过 session.inputNames 查看）
    const inputName = session.inputNames ? session.inputNames[0] : session._modelGraph.initializerNames[0];
    feeds[inputName] = inputTensor;
    const results = await session.run(feeds);

    // 取第一个输出（示例）
    const outName = session.outputNames[0];
    const output = results[outName].data; // Float32Array

    log('Postprocessing...');
    // 需要你根据具体模型写 meta（numClasses / paramsPerBox 等）
    const meta = { numClasses: CLASS_NAMES.length, paramsPerBox: 4 + 1 + CLASS_NAMES.length };

    // 解析（此处用了 parseYOLOOutput 作为示例）
    const parsed = parseYOLOOutput(output, meta);
    // NMS
    const keep = nms(parsed.boxes, parsed.scores, IOU_THRESHOLD);
    const boxes = keep.map(i => parsed.boxes[i]);
    const scores = keep.map(i => parsed.scores[i]);
    const classes = keep.map(i => parsed.classes[i]);

    const dets = { boxes, scores, classes };
    drawDetections(dets, meta, prep);
    log(`Detections: ${dets.boxes.length}`);
  }

  function log(s){ logEl.textContent = s; }

  // 预先加载模型（可选）
  // loadModel(); // 如果你想页面一打开就加载

</script>
</body>
</html>
